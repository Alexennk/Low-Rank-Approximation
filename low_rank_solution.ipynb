{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hkjTSe3QwXY"
      },
      "source": [
        "# Aleksey Senkin - Low-Rank approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEWqgkD692dZ"
      },
      "source": [
        "# Exploring the rank of trained Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3VP30ED-B_g"
      },
      "source": [
        "In this notebook, you're going to explore trained neural networks, and study the rank of its matrices.\n",
        "\n",
        "**Reminder**: The rank is the number of independent columns of the matrix. If a matrix $A \\in \\mathbb{R}^{n\\times m}$  has rank $k$, then $A$ can be approximated by\n",
        "\n",
        "$$A \\approx B \\cdot C$$\n",
        "\n",
        "where $B \\in \\mathbb{R}^{n\\times k}$ and $C \\in \\mathbb{R}^{k\\times m}$.\n",
        "\n",
        "You can find the rank of matrix $A$ by performing Gaussian elimination and counting the number of pivots. This can be done in few lines of `numpy` code.\n",
        "\n",
        "**References**:\n",
        "- https://arxiv.org/pdf/1804.08838\n",
        "- https://arxiv.org/pdf/2209.13569\n",
        "- https://arxiv.org/pdf/2012.13255\n",
        "\n",
        "Note: The references above are not needed to complete this notebook, but reading them might give you additional insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUKDdnCbMLSC"
      },
      "source": [
        "## Important\n",
        "\n",
        "1. For all the training done, make sure to plot things like the loss values and accuracy on each epoch.\n",
        "\n",
        "    - You can either use tensorboard or just make a static matplotlib plot.\n",
        "    \n",
        "2. Don't add biases to the layers in the network, not important for this notebook.\n",
        "3. No need to use Dropout or BatchNorm on the network.\n",
        "4. Remember to use GPUs during the training.\n",
        "5. Always test your hypothesis on both training and testing sets, you might get a surprising result sometimes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlFG5uMx_uPR"
      },
      "source": [
        "## Task 1: Downloading MNIST and Dataloaders\n",
        "\n",
        "Download the MNIST dataset and split into training and testing, and create dataloaders.\n",
        "\n",
        "Link: https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU4hXFXr_xJT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "da5iVKcr7e_G",
        "outputId": "56536c18-c36d-4ae4-c0aa-bdbb9d86ea21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 14939030.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 430412.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1224891.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1368375.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGXS0tMH7e_G",
        "outputId": "4313fd2c-49a0-4e4a-fa35-407a1028c4f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([32, 1, 28, 28])\n",
            "Shape of y: torch.Size([32]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLAtp2bM-W5z"
      },
      "source": [
        "## Task 2: Train a neural network\n",
        "\n",
        "Build a simple Multi-layered Perceptron with ReLU activations, and train it on MNIST until achieving 95% accuracy or higher.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8ikfbxo914-",
        "outputId": "d8c387cf-1b4c-464d-f966-d2cd5f60b8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEusgx517e_K"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7Hv9jc17e_L"
      },
      "outputs": [],
      "source": [
        "def validate_on_test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xET56bl7e_L"
      },
      "outputs": [],
      "source": [
        "def validate_on_train(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDSNrcn1-Cff"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NN().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIPVKxXV7e_L",
        "outputId": "7887bc79-f74d-448c-a356-86047af64476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 96.5%, Avg loss: 0.111752\n",
            "Test Error: \n",
            " Accuracy: 96.1%, Avg loss: 0.124072 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.115141\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.144408 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.083799\n",
            "Test Error: \n",
            " Accuracy: 96.5%, Avg loss: 0.136577 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 3\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    validate_on_train(train_dataloader, model, loss_fn)\n",
        "    validate_on_test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q1K0drgAIPX"
      },
      "source": [
        "## Task 3: Analyze the rank of the matrices in this network\n",
        "\n",
        "Perform experiments and answer the following questions:\n",
        "- What's the average rank of the matrices on all layers?\n",
        "- How does the rank increase as we go to deeper layers?\n",
        "- Try the same MLP, but change the activation function to others ($\\tanh, \\sigma, \\dots$). Do the answers change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtL3NVMOL6yU",
        "outputId": "454fab44-7a4a-4a76-eb64-6bfe0df6a70d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of 1 layer: torch.Size([512, 784])\n",
            "Rank of 1 layer: 512\n",
            "Shape of 2 layer: torch.Size([512, 512])\n",
            "Rank of 2 layer: 511\n",
            "Shape of 3 layer: torch.Size([10, 512])\n",
            "Rank of 3 layer: 10\n"
          ]
        }
      ],
      "source": [
        "ranks = []\n",
        "\n",
        "for i, parameter in enumerate(model.parameters()):\n",
        "    print(f\"Shape of {i + 1} layer: {parameter.shape}\")\n",
        "    rank = torch.linalg.matrix_rank(parameter)\n",
        "    print(f\"Rank of {i + 1} layer: {rank}\")\n",
        "    ranks.append(rank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEwqn1Cf9oJL"
      },
      "source": [
        "### The matrices rank is almost equal to *min(matrix dimensions)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx1hHpIW7e_L"
      },
      "source": [
        "### Change activation function to tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9HYtTlE7e_M",
        "outputId": "c9aac031-0930-4d43-f2a0-9e3d2bc005c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NN_tanh(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_tanh_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=False)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define model\n",
        "class NN_tanh(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 512, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 10, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_tanh_stack(x)\n",
        "        return logits\n",
        "\n",
        "model_tanh = NN_tanh().to(device)\n",
        "print(model_tanh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoVrH9ev7e_M",
        "outputId": "348e9df7-fcce-4daf-adf7-f3964766aa50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.186010\n",
            "Test Error: \n",
            " Accuracy: 94.1%, Avg loss: 0.191365 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 96.5%, Avg loss: 0.109385\n",
            "Test Error: \n",
            " Accuracy: 96.2%, Avg loss: 0.125641 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.078888\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.105623 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 3\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_tanh.parameters(), lr=1e-3)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model_tanh, loss_fn, optimizer)\n",
        "    validate_on_train(train_dataloader, model_tanh, loss_fn)\n",
        "    validate_on_test(test_dataloader, model_tanh, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mIbleLM7e_M",
        "outputId": "5caf008b-e28d-4605-b5d5-dba192ec11f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of 1 layer: torch.Size([512, 784])\n",
            "Rank of 1 layer: 512\n",
            "Shape of 2 layer: torch.Size([512, 512])\n",
            "Rank of 2 layer: 512\n",
            "Shape of 3 layer: torch.Size([10, 512])\n",
            "Rank of 3 layer: 10\n"
          ]
        }
      ],
      "source": [
        "ranks = []\n",
        "\n",
        "for i, parameter in enumerate(model_tanh.parameters()):\n",
        "    print(f\"Shape of {i + 1} layer: {parameter.shape}\")\n",
        "    rank = torch.linalg.matrix_rank(parameter)\n",
        "    print(f\"Rank of {i + 1} layer: {rank}\")\n",
        "    ranks.append(rank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_1BS3am7e_M"
      },
      "source": [
        "### Weight matrices ranks don't change and are still equal to *min(matrix dimensions)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htp37DfHCQKL"
      },
      "source": [
        "## Task 4: Overfit by scaling the MLP\n",
        "\n",
        "1. Create a bigger network and train it on MNIST, to the point of overfitting.\n",
        "2. Now check the rank of the matrices in the network, and answer the same questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ZJwX5SCOx8",
        "outputId": "530c5c53-0844-4ce6-fddd-3c95b81db8c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NN_scaled(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=1024, out_features=512, bias=False)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=512, out_features=256, bias=False)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=256, out_features=10, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define model\n",
        "class NN_scaled(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 1024, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model_scaled = NN_scaled().to(device)\n",
        "print(model_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wF-OQLpp7e_N",
        "outputId": "a46195bf-b8e8-4f36-f1b8-4ac8b6cd1da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.178778\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.176825 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.109358\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.118930 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.066444\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.091504 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.044982\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.085121 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.032451\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.086183 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.038224\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.102648 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.4%, Avg loss: 0.020740\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.088526 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.024210\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.103176 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.031560\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.126000 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.054144\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.160008 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.7%, Avg loss: 0.009083\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.083016 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.8%, Avg loss: 0.006691\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.088152 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.6%, Avg loss: 0.013328\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.112834 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.7%, Avg loss: 0.008379\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.097558 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.8%, Avg loss: 0.007229\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.099574 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.6%, Avg loss: 0.011015\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.108083 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.6%, Avg loss: 0.011743\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.124712 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.7%, Avg loss: 0.008203\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.107375 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.003883\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.088554 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.003899\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.100962 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.003302\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.100227 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.002000\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.094004 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.001971\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.101625 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.003584\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.101199 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.004552\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.109965 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.004840\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.102788 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.002869\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.097701 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.002907\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.094068 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Avg loss: 0.001466\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.105657 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.001990\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.104066 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.8%, Avg loss: 0.007443\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.125577 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.8%, Avg loss: 0.004841\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.114674 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.002670\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.114457 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.001763\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.112115 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.003932\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.126323 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.7%, Avg loss: 0.018704\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.141670 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.003102\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.106931 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.003699\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.115962 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.004727\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.116812 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.8%, Avg loss: 0.007304\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.131507 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.001657\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.099911 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.8%, Avg loss: 0.008022\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.139316 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Avg loss: 0.001553\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.104994 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Avg loss: 0.001210\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.119985 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.004642\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.120559 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.003135\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.111562 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Avg loss: 0.000955\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.108111 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Avg loss: 0.000521\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.112864 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.003072\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.118884 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Avg loss: 0.000372\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.104507 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_scaled.parameters(), lr=1e-4)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model_scaled, loss_fn, optimizer)\n",
        "    validate_on_train(train_dataloader, model_scaled, loss_fn)\n",
        "    validate_on_test(test_dataloader, model_scaled, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBWzlDt77e_N"
      },
      "source": [
        "**Train Error:**\n",
        " Accuracy: 100%, Avg loss: 0.000372\n",
        "\n",
        "**Test Error:**\n",
        " Accuracy: 98.4%, Avg loss: 0.104507  \n",
        "\n",
        " The model has learnt all the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pK0VHy87e_N",
        "outputId": "d6616b67-1c57-4beb-ea36-648f89ce656d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of 1 layer: torch.Size([1024, 784])\n",
            "Rank of 1 layer: 784\n",
            "Shape of 2 layer: torch.Size([1024, 1024])\n",
            "Rank of 2 layer: 1022\n",
            "Shape of 3 layer: torch.Size([512, 1024])\n",
            "Rank of 3 layer: 512\n",
            "Shape of 4 layer: torch.Size([256, 512])\n",
            "Rank of 4 layer: 256\n",
            "Shape of 5 layer: torch.Size([10, 256])\n",
            "Rank of 5 layer: 10\n"
          ]
        }
      ],
      "source": [
        "ranks = []\n",
        "\n",
        "for i, parameter in enumerate(model_scaled.parameters()):\n",
        "    print(f\"Shape of {i + 1} layer: {parameter.shape}\")\n",
        "    rank = torch.linalg.matrix_rank(parameter)\n",
        "    print(f\"Rank of {i + 1} layer: {rank}\")\n",
        "    ranks.append(rank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p97dJqGeEGOi"
      },
      "source": [
        "Despite having 100% accuracy on train set, matrices ranks didn't get lower.\n",
        "This could be connected with lack of parameters needed to get obvious overfitting. Furthermore, the dataset used is not really designed for fully connected networks, so I suppose this structure doesn't get all the needed information from data - that's why it's quite difficult to overfit it properly.\n",
        "\n",
        "Also, the elements of these weight matrices are floating point numbers so it's clear, that the probability of two rows being linearly dependent is low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jSb8aKmMDcB"
      },
      "source": [
        "## Task 5: Approximate low-rank\n",
        "\n",
        "From some of the references given at the beginning, you can realize that trained neural networks have intrinsically low dimensionality (meaning low-rank matrices).\n",
        "\n",
        "In this task, take the overparametrized network already trained from the TASK4 and try to approximate each layer's matrix with a product of two other low-rank matrices?\n",
        "\n",
        "This means, if a layer has a matrix $A \\in\\mathbb{R}^{n\\times m}$, then try to find two matrices $B \\in \\mathbb{R}^{n\\times r}$ and $C \\in \\mathbb{R}^{r\\times m}$ so that $\\lvert {A - B\\cdot C}\\rvert $ is minimized, where $\\lvert x\\rvert$ means the Frobenius norm. You can use a different norm, if you think it makes sense. In order to learn $B$ and $C$, you can do gradient descent-like algorithms, where you alternate between updating $B$ and $C$ on each optimization step.\n",
        "\n",
        "**Ablate**:\n",
        "Try different values for $r$ and analyze how good your approximation is (for e.g, by taking average Frobenius norm across all layers) as you increase $r$. Make a plot with that.\n",
        "\n",
        "Conclude what is the effective rank $r$: the smallest rank such that the approximation of that rank is good enough (meaning the Frobenius norm is smaller than some threshold chosen by you)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LmaAxuF7e_N",
        "outputId": "0341ae09-4cf0-41c0-ea96-c6c0f92e4316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of 1 layer: torch.Size([1024, 784])\n",
            "Shape of 2 layer: torch.Size([1024, 1024])\n",
            "Shape of 3 layer: torch.Size([512, 1024])\n",
            "Shape of 4 layer: torch.Size([256, 512])\n",
            "Shape of 5 layer: torch.Size([10, 256])\n"
          ]
        }
      ],
      "source": [
        "layers = []\n",
        "\n",
        "for i, parameter in enumerate(model_scaled.parameters()):\n",
        "    print(f\"Shape of {i + 1} layer: {parameter.shape}\")\n",
        "    layers.append(parameter.data)\n",
        "\n",
        "layers = layers[:-1] # not to approximate the last layer with already low rank = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0_EzBc97e_N"
      },
      "outputs": [],
      "source": [
        "class LowRankApprox(nn.Module):\n",
        "    def __init__(self, n, m, rank):\n",
        "        super().__init__()\n",
        "        self.B = nn.Linear(n, rank, bias=False)\n",
        "        self.C = nn.Linear(rank, m, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.C(self.B(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUgH6MOcSIV4"
      },
      "outputs": [],
      "source": [
        "def get_approximated_layers(layers, ranks, lr, epochs):\n",
        "    approximated_layers = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        if i != 0:\n",
        "            print()\n",
        "        # print(f\"Approximating for {i+1} layer\")\n",
        "\n",
        "        n, m = layer.shape\n",
        "        rank = ranks[i]\n",
        "        input = torch.from_numpy(np.eye(n)).float().to(device)\n",
        "        label = layer.to(device)\n",
        "\n",
        "        model = LowRankApprox(n, m, rank).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        loss_fn = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            output = model(input)\n",
        "            loss = loss_fn(output, label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # if epoch % 9000 == 0:\n",
        "            #     print(f'Epoch {epoch}: Loss = {loss.item():.6f}')\n",
        "\n",
        "\n",
        "        approximated_layers.append((model.B.weight.detach().T, model.C.weight.detach().T))\n",
        "\n",
        "    return approximated_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APg-pzaVHTz6"
      },
      "outputs": [],
      "source": [
        "def evaluate_approximated_layers(approximated_layers, layers):\n",
        "    for i, layer in enumerate(approximated_layers):\n",
        "        original_matrix = layers[i].cpu().numpy()\n",
        "        approximation = layer[0].cpu().numpy() @ layer[1].cpu().numpy()\n",
        "\n",
        "        n, m = layer[0].shape[0], layer[1].shape[1]\n",
        "        rank = layer[0].shape[1]\n",
        "\n",
        "        print(f\"Layer {i+1}:\")\n",
        "        # print(f\"Shape of A: {original_matrix.shape}\")\n",
        "        # print(f\"Shape of B: {n, rank}\")\n",
        "        # print(f\"Shape of C: {rank, m}\\n\")\n",
        "\n",
        "        # --- Evaluation ---\n",
        "        compression_ratio = (n * m)  / (n * rank + rank * m )\n",
        "        print(f'Compression Ratio: {compression_ratio:.3f}')\n",
        "\n",
        "        error = np.linalg.norm(original_matrix - approximation)\n",
        "        print(f'Approximation Error (Frobenius Norm): {error:.5f}\\n')\n",
        "\n",
        "        # abs_error = np.abs(original_matrix - approximation)\n",
        "        # relative_error = abs_error / (np.abs(original_matrix) + 1e-8)\n",
        "        # percent_error = 100 * np.mean(relative_error)\n",
        "        # print(f'Approximation Error (Avg Percent): {percent_error:.3f}%\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYGkQC-aGDha"
      },
      "source": [
        "### Try some combinations of ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeRzrGnq7e_O"
      },
      "outputs": [],
      "source": [
        "ranks = [[40, 40, 40, 20],\n",
        "         [100, 100, 100, 50],\n",
        "         [150, 150, 150, 80],\n",
        "         [200, 200, 200, 100]]\n",
        "lr = 1e-4\n",
        "epochs = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m9QMhmfY7e_O",
        "outputId": "397d896d-4f44-453f-dc06-11c22f683abb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for ranks = [40, 40, 40, 20]\n",
            "\n",
            "\n",
            "\n",
            "Layer 1:\n",
            "Compression Ratio: 11.101\n",
            "Approximation Error (Frobenius Norm): 22.99904\n",
            "\n",
            "Layer 2:\n",
            "Compression Ratio: 12.800\n",
            "Approximation Error (Frobenius Norm): 22.48495\n",
            "\n",
            "Layer 3:\n",
            "Compression Ratio: 8.533\n",
            "Approximation Error (Frobenius Norm): 13.42279\n",
            "\n",
            "Layer 4:\n",
            "Compression Ratio: 8.533\n",
            "Approximation Error (Frobenius Norm): 8.92651\n",
            "\n",
            "Training for ranks = [100, 100, 100, 50]\n",
            "\n",
            "\n",
            "\n",
            "Layer 1:\n",
            "Compression Ratio: 4.440\n",
            "Approximation Error (Frobenius Norm): 18.84841\n",
            "\n",
            "Layer 2:\n",
            "Compression Ratio: 5.120\n",
            "Approximation Error (Frobenius Norm): 19.29996\n",
            "\n",
            "Layer 3:\n",
            "Compression Ratio: 3.413\n",
            "Approximation Error (Frobenius Norm): 11.17345\n",
            "\n",
            "Layer 4:\n",
            "Compression Ratio: 3.413\n",
            "Approximation Error (Frobenius Norm): 7.50189\n",
            "\n",
            "Training for ranks = [150, 150, 150, 80]\n",
            "\n",
            "\n",
            "\n",
            "Layer 1:\n",
            "Compression Ratio: 2.960\n",
            "Approximation Error (Frobenius Norm): 16.50859\n",
            "\n",
            "Layer 2:\n",
            "Compression Ratio: 3.413\n",
            "Approximation Error (Frobenius Norm): 17.35723\n",
            "\n",
            "Layer 3:\n",
            "Compression Ratio: 2.276\n",
            "Approximation Error (Frobenius Norm): 9.58826\n",
            "\n",
            "Layer 4:\n",
            "Compression Ratio: 2.133\n",
            "Approximation Error (Frobenius Norm): 6.25632\n",
            "\n",
            "Training for ranks = [200, 200, 200, 100]\n",
            "\n",
            "\n",
            "\n",
            "Layer 1:\n",
            "Compression Ratio: 2.220\n",
            "Approximation Error (Frobenius Norm): 14.53319\n",
            "\n",
            "Layer 2:\n",
            "Compression Ratio: 2.560\n",
            "Approximation Error (Frobenius Norm): 15.63410\n",
            "\n",
            "Layer 3:\n",
            "Compression Ratio: 1.707\n",
            "Approximation Error (Frobenius Norm): 8.13203\n",
            "\n",
            "Layer 4:\n",
            "Compression Ratio: 1.707\n",
            "Approximation Error (Frobenius Norm): 5.48526\n",
            "\n"
          ]
        }
      ],
      "source": [
        "appr_layers = []\n",
        "\n",
        "for i in range(4):\n",
        "    print(f\"Training for ranks = {ranks[i]}\")\n",
        "    approximated_layers = get_approximated_layers(layers, ranks[i], lr, epochs)\n",
        "    appr_layers.append(approximated_layers)\n",
        "    evaluate_approximated_layers(approximated_layers, layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LI_WkjsKJiy"
      },
      "source": [
        "### Bigger hidden ranks expectedly give lower Frobenius norm values, though compression ratio also becomes lower\n",
        "\n",
        "So the chosen effective ranks are [150, 150, 150, 80]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrBwf5bwPOny"
      },
      "source": [
        "## Task 6: Learning with low-rank factorization\n",
        "\n",
        "Once you found the effective rank $r$, take the same architecture from the previous task, and now replace each layer $A \\in \\mathbb{R}^{n\\times m}$ by a layer that applies $B\\cdot C$ with $B\\in \\mathbb{R}^{n\\times r}$ and $C \\in \\mathbb{R}^{r\\times m}$.\n",
        "\n",
        "**Question**: How much memory do you save? (you can just count the number of parameters of the original network and compare to that of the new network).\n",
        "\n",
        "Initialize these values with standard initialization, and train this network.\n",
        "\n",
        "**Question**: How does the learning change? Does it converge faster or slower? What about accuracy on both training and testing sets?\n",
        "\n",
        "**Question**: Now try doing inference, how much improvement do you see?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKg2jvN-7e_O",
        "outputId": "d3feed7e-0ad8-4e66-df3e-8aeb6252c402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1024, 150]) torch.Size([150, 784])\n",
            "torch.Size([1024, 150]) torch.Size([150, 1024])\n",
            "torch.Size([512, 150]) torch.Size([150, 1024])\n",
            "torch.Size([256, 80]) torch.Size([80, 512])\n"
          ]
        }
      ],
      "source": [
        "for layer in appr_layers[2]:\n",
        "    print(layer[0].shape, layer[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_oFw7TlRHFs",
        "outputId": "2df99a7a-5899-43c1-adab-399111e66a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NN_approximated(\n",
            "  (activation): ReLU()\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (B1): Linear(in_features=150, out_features=1024, bias=False)\n",
            "  (C1): Linear(in_features=784, out_features=150, bias=False)\n",
            "  (B2): Linear(in_features=150, out_features=1024, bias=False)\n",
            "  (C2): Linear(in_features=1024, out_features=150, bias=False)\n",
            "  (B3): Linear(in_features=150, out_features=512, bias=False)\n",
            "  (C3): Linear(in_features=1024, out_features=150, bias=False)\n",
            "  (B4): Linear(in_features=80, out_features=256, bias=False)\n",
            "  (C4): Linear(in_features=512, out_features=80, bias=False)\n",
            "  (A5): Linear(in_features=256, out_features=10, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define model\n",
        "class NN_approximated(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.activation = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.B1 = nn.Linear(150, 1024, bias=False)\n",
        "        self.C1 = nn.Linear(784, 150, bias=False)\n",
        "\n",
        "        self.B2 = nn.Linear(150, 1024, bias=False)\n",
        "        self.C2 = nn.Linear(1024, 150, bias=False)\n",
        "\n",
        "        self.B3 = nn.Linear(150, 512, bias=False)\n",
        "        self.C3 = nn.Linear(1024, 150, bias=False)\n",
        "\n",
        "        self.B4 = nn.Linear(80, 256, bias=False)\n",
        "        self.C4 = nn.Linear(512, 80, bias=False)\n",
        "\n",
        "        self.A5 = nn.Linear(256, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        a1 = self.activation(self.B1(self.C1(x)))\n",
        "\n",
        "        a2 = self.activation(self.B2(self.C2(a1)))\n",
        "        a3 = self.activation(self.B3(self.C3(a2)))\n",
        "        a4 = self.activation(self.B4(self.C4(a3)))\n",
        "\n",
        "        logits = self.A5(a4)\n",
        "        return logits\n",
        "\n",
        "model_approximated = NN_approximated().to(device)\n",
        "print(model_approximated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AazRxK1L7e_T"
      },
      "source": [
        "### Compare low-rank and basic models' performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iP9O7aj7e_T"
      },
      "outputs": [],
      "source": [
        "# low-rank\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_approximated.parameters(), lr=0.5e-3)\n",
        "\n",
        "epochs = 15\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model_approximated, loss_fn, optimizer)\n",
        "    validate_on_train(train_dataloader, model_approximated, loss_fn)\n",
        "    validate_on_test(test_dataloader, model_approximated, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoCNeeJFNPT7"
      },
      "outputs": [],
      "source": [
        "# basic\n",
        "\n",
        "model_scaled = NN_scaled().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_scaled.parameters(), lr=0.5e-3)\n",
        "\n",
        "epochs = 15\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model_scaled, loss_fn, optimizer)\n",
        "    validate_on_train(train_dataloader, model_scaled, loss_fn)\n",
        "    validate_on_test(test_dataloader, model_scaled, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_WMNSVK7e_T"
      },
      "source": [
        "### With *epochs = 15, lr = 0.5e-3* we get the next results\n",
        "\n",
        "Model with low-rank approximation *(ranks = [150, 150, 150, 80])*:\n",
        "- Time for training: 4m 0s\n",
        "- Train loss: 0.02393\n",
        "- Train accuracy: 99.3%\n",
        "- Test loss: 0.133316\n",
        "- Test accuracy: 97.8%\n",
        "\n",
        "Basic model:\n",
        "- Time for training: 5m 17s\n",
        "- Train loss: 0.007569\n",
        "- Train accuracy: 99.8%\n",
        "- Test loss: 0.094549\n",
        "- Test accuracy: 98.3%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC8_kN5K7e_T"
      },
      "source": [
        "### Compare the size of two models (total number of parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRVE7nT17e_T",
        "outputId": "62c5139e-c7ec-4744-8b44-87947e2f3a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The low-rank model has 872,800 trainable parameters\n",
            "The basic model has 2,509,312 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The low-rank model has {count_parameters(model_approximated):,} trainable parameters')\n",
        "print(f'The basic model has {count_parameters(model_scaled):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S61ugfyu7e_U"
      },
      "source": [
        "### As we can see, the new model has 3 times less parameters than the basic one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpYV1ebrkma1"
      },
      "source": [
        "## Task 7: Final conclusions\n",
        "\n",
        "Based on all the previous experiments, report your conclusions and try to give an explanation to the behaviours you observed.\n",
        "\n",
        "Can you think of other ways of using the low-rank factorizations? What about SVD? Provide an explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0sK2e9dlMY-"
      },
      "source": [
        "### Some conclusions about low-rank factorizations:\n",
        "\n",
        "- Overfitting leads to lower ranks of weight matrices in a Neural Network. This is not a surprise, because overfitted models usually tend to find more complex dependencies, than the real ones existing between train sample and target. This is the result of the bigger amount of parameters and complex structure of the model; and in this case lower rank of weight matrices may point at the excessive amount of parameters.\n",
        "- This method fastens training and inference processes of the model while maintaining the overall quality of a similar model with a bigger amount of parameters. The choice of hidden rank affects the model quality.\n",
        "- The main application of low-rank factorization is reducing the number of parameters in a neural network and, hence, reducing it's computational cost (both memory consumption and time of processing).\n",
        "- Speaking about SVD, it is an optimal (in terms of Frobenius norm) way to get a low-rank approximation of a given matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgH6pfFNQ196"
      },
      "source": [
        "## BONUS Task: LoRA\n",
        "\n",
        "Propose ideas by which low-rank could improve fine-tuning and training? Which disadvantages does it have?\n",
        "\n",
        "Read about LoRA (given in one of the references at the begining of the notebook).\n",
        "\n",
        "Now, take MNIST, and remove some digit from the dataset (keep the same labels, just remove the datapoints of a specific label).\n",
        "\n",
        "Train a simple MLP on this modified dataset.\n",
        "Fine-tune in the datapoints of the chosen digit, by using LoRA.\n",
        "\n",
        "Report the memory and time overheads."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
